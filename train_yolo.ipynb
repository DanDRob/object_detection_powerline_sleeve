{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import necessary libraries and define the path to the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 Training Notebook\\\\n\"\n",
    "This notebook trains a YOLOv8 model using a configuration defined in `config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# YOLOv8 Training Script\n",
    "#\n",
    "# This script trains a YOLOv8 model using a configuration defined in `config.yaml`.\n",
    "# It replicates the steps typically found in a Jupyter Notebook for training.\n",
    "\n",
    "# =============================================================================\n",
    "# ## 1. Setup\n",
    "#\n",
    "# Import necessary libraries and define the path to the configuration file.\n",
    "# =============================================================================\n",
    "import yaml\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch # Optional: Check GPU availability\n",
    "import json # For potentially printing config nicely\n",
    "\n",
    "CONFIG_PATH = \"config.yaml\"\n",
    "\n",
    "# =============================================================================\n",
    "# ## 2. Load Configuration\n",
    "#\n",
    "# Load training parameters from the `config.yaml` file.\n",
    "# =============================================================================\n",
    "config = None\n",
    "dataset_yaml = None\n",
    "model_type = None\n",
    "hyperparams = {}\n",
    "output_config = {}\n",
    "device = None\n",
    "\n",
    "print(\"--- Loading Configuration ---\")\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at {CONFIG_PATH}\")\n",
    "    # Exit or provide default values if needed\n",
    "    exit(1) # Exit if config is essential\n",
    "except Exception as e:\n",
    "    print(f\"Error loading configuration: {e}\")\n",
    "    exit(1) # Exit on other loading errors\n",
    "\n",
    "if config:\n",
    "    print(\"Configuration loaded successfully:\")\n",
    "    # Optional: Print loaded config for verification\n",
    "    # print(json.dumps(config, indent=2))\n",
    "\n",
    "    # --- Extract key parameters ---\n",
    "    dataset_yaml = config.get('dataset_yaml_path')\n",
    "    model_type = config.get('model_type')\n",
    "    hyperparams = config.get('hyperparameters', {}) # Default to empty dict\n",
    "    output_config = config.get('output', {})       # Default to empty dict\n",
    "    device = config.get('device')\n",
    "\n",
    "    # --- Basic Validation ---\n",
    "    valid_config = True\n",
    "    if not dataset_yaml or not os.path.exists(dataset_yaml):\n",
    "        print(f\"Error: Dataset YAML path '{dataset_yaml}' is invalid or file does not exist. Please check config.yaml.\")\n",
    "        valid_config = False\n",
    "    if not model_type:\n",
    "        print(\"Error: 'model_type' not specified in config.yaml.\")\n",
    "        valid_config = False\n",
    "    if not output_config.get('project_name') or not output_config.get('experiment_name'):\n",
    "        print(\"Error: 'project_name' or 'experiment_name' missing in output configuration.\")\n",
    "        valid_config = False\n",
    "\n",
    "    if not valid_config:\n",
    "        print(\"Configuration errors detected. Exiting.\")\n",
    "        exit(1)\n",
    "else:\n",
    "    # This case should ideally be caught by the exit(1) above, but included for completeness\n",
    "    print(\"Configuration could not be loaded. Exiting.\")\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ## 3. Environment Check (Optional)\n",
    "#\n",
    "# Verify GPU availability if specified.\n",
    "# =============================================================================\n",
    "print(\"\\n--- Environment Check ---\")\n",
    "if device and device != 'cpu':\n",
    "    try:\n",
    "        print(f\"Attempting to use device: {device}\")\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        print(f\"PyTorch CUDA available: {gpu_available}\")\n",
    "        if not gpu_available:\n",
    "             print(f\"Warning: Device set to '{device}' but CUDA is not available. Training might default to CPU or fail.\")\n",
    "             # Optionally force device to 'cpu' or exit if GPU is mandatory\n",
    "             # device = 'cpu'\n",
    "             # print(\"Forcing device to CPU.\")\n",
    "        else:\n",
    "            print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "            # You could add more specific checks for the requested device ID here\n",
    "            # e.g., check if device string is '0', '1', etc. and if that device exists\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPU check: {e}\")\n",
    "else:\n",
    "    print(f\"Device specified as: {device if device else 'Default (likely CPU if no GPU)'}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ## 4. Initialize Model\n",
    "#\n",
    "# Load the specified YOLOv8 model. This could be a pre-trained model\n",
    "# (like `yolov8n.pt`) or a path to a checkpoint from previous training.\n",
    "# =============================================================================\n",
    "print(\"\\n--- Initializing Model ---\")\n",
    "model = None\n",
    "try:\n",
    "    print(f\"Initializing model: {model_type}\")\n",
    "    model = YOLO(model_type)\n",
    "    print(\"Model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing YOLO model: {e}\")\n",
    "    print(\"Exiting.\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ## 5. Start Training\n",
    "#\n",
    "# Run the YOLOv8 training process using the loaded configuration.\n",
    "# =============================================================================\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "results = None\n",
    "if model: # Redundant check if exits above are used, but safe\n",
    "    try:\n",
    "        # Prepare keyword arguments, filtering out None values if necessary\n",
    "        train_args = {\n",
    "            'data': dataset_yaml,\n",
    "            'epochs': hyperparams.get('epochs', 100), # Default if not in config\n",
    "            'imgsz': hyperparams.get('image_size', 640),\n",
    "            'batch': hyperparams.get('batch_size', 16),\n",
    "            'project': output_config.get('project_name'),\n",
    "            'name': output_config.get('experiment_name'),\n",
    "            'device': device,\n",
    "            # Add other hyperparameters dynamically\n",
    "            **{k: v for k, v in hyperparams.items() if k not in ['epochs', 'image_size', 'batch_size'] and v is not None}\n",
    "        }\n",
    "        print(\"Training arguments:\")\n",
    "        print(json.dumps(train_args, indent=2))\n",
    "\n",
    "        results = model.train(**train_args)\n",
    "\n",
    "        print(\"\\nTraining finished.\")\n",
    "        # Results object might not always have save_dir if training interrupted early\n",
    "        if hasattr(results, 'save_dir'):\n",
    "            print(f\"Results saved to: {results.save_dir}\")\n",
    "        else:\n",
    "             print(\"Training completed, but couldn't access results.save_dir (may have been interrupted or an older ultralytics version behavior). Check project directory.\")\n",
    "             print(f\"Project: {output_config.get('project_name')}, Name: {output_config.get('experiment_name')}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        # Consider logging the full traceback here for debugging\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    # Should not be reachable if exits are used correctly\n",
    "    print(\"Skipping training due to model initialization failure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# ## 6. Evaluation (Optional)\n",
    "#\n",
    "# Evaluate the best performing model checkpoint on the validation or test set.\n",
    "# =============================================================================\n",
    "print(\"\\n--- Evaluation ---\")\n",
    "if results and hasattr(results, 'save_dir'):\n",
    "    # Path to the best model weights saved during training\n",
    "    best_model_path = os.path.join(results.save_dir, 'weights/best.pt')\n",
    "\n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Loading best model from: {best_model_path}\")\n",
    "        try:\n",
    "            best_model = YOLO(best_model_path)\n",
    "\n",
    "            print(\"\\nEvaluating on the validation set...\")\n",
    "            # The 'val' split is typically defined in your data.yaml\n",
    "            # Ensure device is passed if needed for consistency\n",
    "            val_metrics = best_model.val(split='val', device=device)\n",
    "            print(\"Validation Metrics obtained.\")\n",
    "            # Accessing specific metrics might depend on the task (detect, segment, classify)\n",
    "            # Example for detection:\n",
    "            if hasattr(val_metrics, 'box'):\n",
    "                 print(f\"Validation mAP50-95: {val_metrics.box.map}\")\n",
    "                 print(f\"Validation mAP50: {val_metrics.box.map50}\")\n",
    "\n",
    "            # --- Evaluate on Test Set (if defined in data.yaml) ---\n",
    "            # Check if 'test' split exists in the dataset yaml\n",
    "            # Accessing model.trainer.args.data is one way, but might change or not exist if only loaded\n",
    "            data_cfg_path = dataset_yaml # Use the path from our config\n",
    "            test_split_exists = False\n",
    "            test_path = None\n",
    "            if isinstance(data_cfg_path, str) and os.path.exists(data_cfg_path):\n",
    "                try:\n",
    "                    with open(data_cfg_path, 'r') as f:\n",
    "                        d_yaml = yaml.safe_load(f)\n",
    "                        if 'test' in d_yaml and d_yaml['test']:\n",
    "                            test_path = d_yaml['test']\n",
    "                            # Basic check if the path seems valid (not exhaustive)\n",
    "                            if isinstance(test_path, str) and test_path.strip():\n",
    "                                test_split_exists = True\n",
    "                                print(f\"Test split found in {data_cfg_path}: {test_path}\")\n",
    "                            else:\n",
    "                                print(f\"Test split key found in {data_cfg_path}, but the value is empty or invalid.\")\n",
    "                        else:\n",
    "                             print(f\"'test' key not found or empty in {data_cfg_path}.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                     print(f\"Could not read or parse data yaml {data_cfg_path} to check for test set: {e}\")\n",
    "            else:\n",
    "                 print(f\"Dataset YAML path '{data_cfg_path}' not found, cannot check for test set.\")\n",
    "\n",
    "\n",
    "            if test_split_exists:\n",
    "                 print(\"\\nEvaluating on the test set...\")\n",
    "                 try:\n",
    "                    # Ensure device is passed if needed\n",
    "                    test_metrics = best_model.val(split='test', device=device)\n",
    "                    print(\"Test Metrics obtained.\")\n",
    "                    # Example for detection:\n",
    "                    if hasattr(test_metrics, 'box'):\n",
    "                        print(f\"Test mAP50-95: {test_metrics.box.map}\")\n",
    "                        print(f\"Test mAP50: {test_metrics.box.map50}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Error during test set evaluation: {e}\")\n",
    "                     # This might happen if the test set path in yaml is invalid or data is corrupt\n",
    "            else:\n",
    "                print(\"\\nTest set not defined or found in dataset YAML. Skipping test set evaluation.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or evaluating best model: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Best model weights not found at {best_model_path}\")\n",
    "elif results is None:\n",
    "    print(\"Skipping evaluation because training did not run or failed.\")\n",
    "else:\n",
    "    print(\"Skipping evaluation because training results object did not contain expected 'save_dir'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ## 7. Next Steps / Further Analysis\n",
    "#\n",
    "# Consider:\n",
    "# *   Analyzing the generated plots (e.g., confusion matrix, PR curves)\n",
    "#     in the `results.save_dir` (if available). The specific directory is:\n",
    "#     `{output_config.get('project_name')}/{output_config.get('experiment_name')}`\n",
    "# *   Performing inference on new images using the trained model\n",
    "#     (`best_model.predict(...)` if evaluation ran successfully).\n",
    "# *   Adjusting hyperparameters in `config.yaml` and re-running the training.\n",
    "# *   Switching datasets by changing `dataset_yaml_path` in `config.yaml`.\n",
    "# =============================================================================\n",
    "print(\"\\n--- Script Finished ---\")\n",
    "if results and hasattr(results, 'save_dir'):\n",
    "    print(f\"Check the output directory for detailed results and plots: {results.save_dir}\")\n",
    "elif output_config.get('project_name') and output_config.get('experiment_name'):\n",
    "     potential_dir = os.path.join(output_config.get('project_name', 'runs/detect'), output_config.get('experiment_name'))\n",
    "     print(f\"Training may have saved results in or near: {potential_dir}\")\n",
    "else:\n",
    "     print(\"Output directory information not fully available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
